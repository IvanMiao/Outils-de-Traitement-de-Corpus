1. distilled bert multilingual:

Map: 100%|███████████████████████████████████████████████████| 3738/3738 [00:00<00:00, 20808.47 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 1.8123, 'grad_norm': 9.5427885055542, 'learning_rate': 1.9239188672024493e-05, 'epoch': 0.11}     
{'loss': 1.7152, 'grad_norm': 11.027336120605469, 'learning_rate': 1.8475315729047075e-05, 'epoch': 0.23}  
{'loss': 1.6424, 'grad_norm': 9.568676948547363, 'learning_rate': 1.77099119785687e-05, 'epoch': 0.34}     
{'loss': 1.5912, 'grad_norm': 10.19856071472168, 'learning_rate': 1.6944508228090318e-05, 'epoch': 0.46}   
{'loss': 1.5733, 'grad_norm': 4.863920211791992, 'learning_rate': 1.617910447761194e-05, 'epoch': 0.57}    
{'loss': 1.5433, 'grad_norm': 8.946239471435547, 'learning_rate': 1.5413700727133565e-05, 'epoch': 0.69}   
{'loss': 1.5054, 'grad_norm': 7.729686260223389, 'learning_rate': 1.4648296976655188e-05, 'epoch': 0.8}    
{'loss': 1.4983, 'grad_norm': 16.829587936401367, 'learning_rate': 1.388289322617681e-05, 'epoch': 0.92}   
{'eval_loss': 1.4935048818588257, 'eval_accuracy': 0.37744441467988216, 'eval_runtime': 147.9242, 'eval_samples_per_second': 25.236, 'eval_steps_per_second': 6.314, 'epoch': 1.0}                                    
{'loss': 1.4555, 'grad_norm': 8.604893684387207, 'learning_rate': 1.3117489475698432e-05, 'epoch': 1.03}   
{'loss': 1.3828, 'grad_norm': 16.4444637298584, 'learning_rate': 1.2353616532721011e-05, 'epoch': 1.15}    
{'loss': 1.3546, 'grad_norm': 11.717642784118652, 'learning_rate': 1.1588212782242635e-05, 'epoch': 1.26}  
{'loss': 1.3518, 'grad_norm': 42.68764877319336, 'learning_rate': 1.0822809031764256e-05, 'epoch': 1.38}   
{'loss': 1.3743, 'grad_norm': 13.475960731506348, 'learning_rate': 1.005740528128588e-05, 'epoch': 1.49}   
{'loss': 1.3443, 'grad_norm': 12.869536399841309, 'learning_rate': 9.293532338308458e-06, 'epoch': 1.61}   
{'loss': 1.3323, 'grad_norm': 12.714449882507324, 'learning_rate': 8.529659395331037e-06, 'epoch': 1.72}   
{'loss': 1.3166, 'grad_norm': 21.881832122802734, 'learning_rate': 7.76425564485266e-06, 'epoch': 1.84}    
{'loss': 1.3417, 'grad_norm': 23.684083938598633, 'learning_rate': 6.998851894374284e-06, 'epoch': 1.95}   
{'eval_loss': 1.4148069620132446, 'eval_accuracy': 0.4186980980444683, 'eval_runtime': 146.3321, 'eval_samples_per_second': 25.51, 'eval_steps_per_second': 6.383, 'epoch': 2.0}                                      
{'loss': 1.2372, 'grad_norm': 20.65162467956543, 'learning_rate': 6.233448143895906e-06, 'epoch': 2.07}    
{'loss': 1.1696, 'grad_norm': 23.054729461669922, 'learning_rate': 5.468044393417528e-06, 'epoch': 2.18}   
{'loss': 1.1505, 'grad_norm': 29.503507614135742, 'learning_rate': 4.702640642939151e-06, 'epoch': 2.3}    
{'loss': 1.1057, 'grad_norm': 19.399494171142578, 'learning_rate': 3.937236892460774e-06, 'epoch': 2.41}   
{'loss': 1.1649, 'grad_norm': 34.19526290893555, 'learning_rate': 3.171833141982396e-06, 'epoch': 2.53}    
{'loss': 1.1158, 'grad_norm': 15.489434242248535, 'learning_rate': 2.4079601990049753e-06, 'epoch': 2.64}  
{'loss': 1.1649, 'grad_norm': 20.93309783935547, 'learning_rate': 1.642556448526598e-06, 'epoch': 2.76}    
{'loss': 1.1338, 'grad_norm': 12.536033630371094, 'learning_rate': 8.771526980482205e-07, 'epoch': 2.87}   
{'loss': 1.1227, 'grad_norm': 23.84526252746582, 'learning_rate': 1.1327975507079985e-07, 'epoch': 2.99}   
{'eval_loss': 1.4760122299194336, 'eval_accuracy': 0.42218055183498526, 'eval_runtime': 146.3477, 'eval_samples_per_second': 25.508, 'eval_steps_per_second': 6.382, 'epoch': 3.0}                                    
{'train_runtime': 7439.9729, 'train_samples_per_second': 7.023, 'train_steps_per_second': 1.756, 'train_loss': 1.3640655809556483, 'epoch': 3.0}


test: 
Result: {'test_loss': 1.3935350179672241, 'test_model_preparation_time': 0.001, 'test_accuracy': 0.42482611021936867, 'test_runtime': 118.6473, 'test_samples_per_second': 31.505, 'test_steps_per_second': 3.944}

Accuracy for each Dynasty:
WeiJin: 82.48% (193/234)
NanBei: 53.97% (170/315)
Tang: 36.80% (209/568)
Song: 12.11% (54/446)
Yuan: 51.15% (421/823)
Ming: 36.32% (284/782)
Qing: 45.09% (257/570)

Report of classification:
              precision    recall  f1-score   support

      WeiJin     0.5627    0.8248    0.6690       234
      NanBei     0.5986    0.5397    0.5676       315
        Tang     0.4953    0.3680    0.4222       568
        Song     0.3553    0.1211    0.1806       446
        Yuan     0.4140    0.5115    0.4576       823
        Ming     0.3389    0.3632    0.3506       782
        Qing     0.3768    0.4509    0.4105       570

    accuracy                         0.4248      3738
   macro avg     0.4488    0.4542    0.4369      3738
weighted avg     0.4228    0.4248    0.4121      3738


2. bert-base-chinese:

Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 1.7852, 'grad_norm': 13.974242210388184, 'learning_rate': 1.8291422666207374e-05, 'epoch': 0.17}  
{'loss': 1.6555, 'grad_norm': 36.89815902709961, 'learning_rate': 1.6572511195315194e-05, 'epoch': 0.34}   
{'loss': 1.6018, 'grad_norm': 11.818793296813965, 'learning_rate': 1.4850155012056495e-05, 'epoch': 0.52}  
{'loss': 1.5566, 'grad_norm': 8.199505805969238, 'learning_rate': 1.3127798828797798e-05, 'epoch': 0.69}   
{'loss': 1.5164, 'grad_norm': 18.887300491333008, 'learning_rate': 1.1405442645539097e-05, 'epoch': 0.86}  
{'eval_loss': 1.4910613298416138, 'eval_accuracy': 0.3680685775515671, 'eval_runtime': 1073.0709, 'eval_samples_per_second': 3.479, 'eval_steps_per_second': 0.581, 'epoch': 1.0}                                     
{'loss': 1.4426, 'grad_norm': 13.89853286743164, 'learning_rate': 9.6830864622804e-06, 'epoch': 1.03}      
{'loss': 1.3377, 'grad_norm': 38.704444885253906, 'learning_rate': 7.960730279021703e-06, 'epoch': 1.21}   
{'loss': 1.3184, 'grad_norm': 27.84832763671875, 'learning_rate': 6.238374095763005e-06, 'epoch': 1.38}    
{'loss': 1.3148, 'grad_norm': 16.692171096801758, 'learning_rate': 4.516017912504307e-06, 'epoch': 1.55}   
{'loss': 1.2707, 'grad_norm': 14.060258865356445, 'learning_rate': 2.7936617292456082e-06, 'epoch': 1.72}  
{'loss': 1.2533, 'grad_norm': 33.027015686035156, 'learning_rate': 1.0713055459869102e-06, 'epoch': 1.89}  
{'eval_loss': 1.4106481075286865, 'eval_accuracy': 0.4240557192606483, 'eval_runtime': 1120.8252, 'eval_samples_per_second': 3.331, 'eval_steps_per_second': 0.556, 'epoch': 2.0}                                     
{'train_runtime': 33206.7151, 'train_samples_per_second': 1.049, 'train_steps_per_second': 0.175, 'train_loss': 1.4508439818621093, 'epoch': 2.0}                                                                     
100%|████████████████████████████████████████████████████████████████| 5806/5806 [9:13:26<00:00,  5.72s/it]

test:
Result: {'test_loss': 1.3762357234954834, 'test_model_preparation_time': 0.0021, 'test_accuracy': 0.4462279293739968, 'test_runtime': 235.66, 'test_samples_per_second': 15.862, 'test_steps_per_second': 1.986}


Accuracy for each Dynasty:
WeiJin: 73.50% (172/234)
NanBei: 60.32% (190/315)
Tang: 58.98% (335/568)
Song: 14.13% (63/446)
Yuan: 53.22% (438/823)
Ming: 40.03% (313/782)
Qing: 27.54% (157/570)

Report of classification:
              precision    recall  f1-score   support

      WeiJin     0.6565    0.7350    0.6935       234
      NanBei     0.5994    0.6032    0.6013       315
        Tang     0.4110    0.5898    0.4845       568
        Song     0.3103    0.1413    0.1941       446
        Yuan     0.4183    0.5322    0.4684       823
        Ming     0.3789    0.4003    0.3893       782
        Qing     0.5858    0.2754    0.3747       570

    accuracy                         0.4462      3738
   macro avg     0.4800    0.4682    0.4580      3738
weighted avg     0.4518    0.4462    0.4326      3738


3. guwenbert

You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model.safetensors: 100%|█████████████████████████████████████████████████| 418M/418M [00:32<00:00, 12.8MB/s]
{'loss': 1.677, 'grad_norm': 25.22924041748047, 'learning_rate': 1.829831209094041e-05, 'epoch': 0.17}      
{'loss': 1.5626, 'grad_norm': 11.877179145812988, 'learning_rate': 1.6575955907681708e-05, 'epoch': 0.34}   
{'loss': 1.4769, 'grad_norm': 8.981181144714355, 'learning_rate': 1.485359972442301e-05, 'epoch': 0.52}     
{'loss': 1.4279, 'grad_norm': 13.33908462524414, 'learning_rate': 1.3131243541164313e-05, 'epoch': 0.69}    
{'loss': 1.3749, 'grad_norm': 36.327659606933594, 'learning_rate': 1.1408887357905616e-05, 'epoch': 0.86} 
{'eval_loss': 1.3528465032577515, 'eval_accuracy': 0.4379855344227163, 'eval_runtime': 1087.3327, 'eval_samples_per_second': 3.433, 'eval_steps_per_second': 0.573, 'epoch': 1.0}  
{'loss': 1.3334, 'grad_norm': 18.098678588867188, 'learning_rate': 9.686531174646919e-06, 'epoch': 1.03}        {'loss': 1.2207, 'grad_norm': 23.3232421875, 'learning_rate': 7.96417499138822e-06, 'epoch': 1.21}              
{'loss': 1.2089, 'grad_norm': 32.47312545776367, 'learning_rate': 6.241818808129522e-06, 'epoch': 1.38}         
{'loss': 1.2195, 'grad_norm': 20.96390724182129, 'learning_rate': 4.519462624870823e-06, 'epoch': 1.55}         
{'loss': 1.1771, 'grad_norm': 21.345985412597656, 'learning_rate': 2.800551153978643e-06, 'epoch': 1.72}        
{'loss': 1.1501, 'grad_norm': 26.801237106323242, 'learning_rate': 1.078194970719945e-06, 'epoch': 1.89}        
{'eval_loss': 1.2604148387908936, 'eval_accuracy': 0.47897133672649345, 'eval_runtime': 1074.2435, 'eval_samples_per_second': 3.475, 'eval_steps_per_second': 0.58, 'epoch': 2.0}                                               
{'train_runtime': 33433.4179, 'train_samples_per_second': 1.042, 'train_steps_per_second': 0.174, 'train_loss': 1.3396599139175125, 'epoch': 2.0} 

test:
Evaluating the model on test dataset...
100%|█████████████████████████████████████████████████████████████████████████| 234/234 [22:31<00:00,  5.77s/it]
Result: {'test_loss': 1.2290503978729248, 'test_model_preparation_time': 0.0021, 'test_accuracy': 0.49143927233814877, 'test_runtime': 1356.204, 'test_samples_per_second': 2.756, 'test_steps_per_second': 0.173}

Accuracy for each Dynasty:
WeiJin:83.76% (196/234)
NanBei:64.13% (202/315)
Tang:55.46% (315/568)
Song:22.87% (102/446)
Yuan:68.77% (566/823)
Ming:43.61% (341/782)
Qing:20.18% (115/570)

Report of classification:
              precision    recall  f1-score   support

      WeiJin     0.7626    0.8376    0.7984       234
      NanBei     0.7953    0.6413    0.7100       315
        Tang     0.5686    0.5546    0.5615       568
        Song     0.4163    0.2287    0.2952       446
        Yuan     0.4060    0.6877    0.5106       823
        Ming     0.3853    0.4361    0.4091       782
        Qing     0.7718    0.2018    0.3199       570

    accuracy                         0.4914      3738
   macro avg     0.5866    0.5125    0.5150      3738
weighted avg     0.5385    0.4914    0.4771      3738
